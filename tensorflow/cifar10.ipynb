{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "初始化权重函数\n",
    "'''\n",
    "def variable_with_weight_loss(shape, std, w1):\n",
    "    var = tf.Variable(tf.truncated_normal(shape,stddev=std),dtype=tf.float32)\n",
    "    \n",
    "    if w1 is not None :\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name=\"weight_loss\")\n",
    "        tf.add_to_collection(\"losses\",weight_loss)\n",
    "    return var\n",
    "\n",
    "'''\n",
    "损失函数\n",
    "'''\n",
    "def loss_func(logits,labels):\n",
    "    labels = tf.cast(labels,tf.int32)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                           labels=labels,name=\"cross_entropy_per_example\")\n",
    "    cross_entropy_mean = tf.reduce_mean(tf.reduce_sum(cross_entropy))\n",
    "    tf.add_to_collection(\"losses\",cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection(\"losses\"),name=\"total_loss\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def forward(image_holder,train):\n",
    "    \n",
    "    #设计第一层卷积\n",
    "    weight1 = variable_with_weight_loss(shape=[5,5,3,64],std=5e-2,w1=0)\n",
    "    kernel1 = tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding=\"SAME\")\n",
    "    bais1 = tf.Variable(tf.constant(0.0,dtype=tf.float32,shape=[64]))\n",
    "    conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bais1))\n",
    "    pool1 = tf.nn.max_pool(conv1,[1,3,3,1],[1,2,2,1],padding=\"SAME\")\n",
    "    norm1 = tf.nn.lrn(pool1,4,bias=1.0,alpha=0.001 / 9,beta=0.75)\n",
    "\n",
    "    #设计第二层卷积\n",
    "    weight2 = variable_with_weight_loss(shape=[5,5,64,64],std=5e-2,w1=0)\n",
    "    kernel2 = tf.nn.conv2d(norm1,weight2,[1,1,1,1],padding=\"SAME\")\n",
    "    bais2 = tf.Variable(tf.constant(0.1,dtype=tf.float32,shape=[64]))\n",
    "    conv2 = tf.nn.relu(tf.nn.bias_add(kernel2,bais2))\n",
    "    norm2 = tf.nn.lrn(conv2,4,bias=1.0,alpha=0.01 / 9,beta=0.75)\n",
    "    pool2 = tf.nn.max_pool(norm2,[1,3,3,1],[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "    #第一层全连接层\n",
    "    # 把pool2从三维张量变为二维张量\n",
    "    pool_shape = pool2.get_shape().as_list()         # 得到pool2 输出矩阵的维度存入list中\n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]      # 提取特征的长度宽度深度 相乘得所有特征点的个数\n",
    "    reshape = tf.reshape(pool2, [pool_shape[0], nodes])       # pool_shape[0]是一个batch的值\n",
    "    #reshape = tf.reshape(pool2,[batch_size,-1])\n",
    "    #dim = reshape.get_shape()[1].value\n",
    "    weight3 = variable_with_weight_loss([nodes,384],std=0.04,w1=0.004)\n",
    "    bais3 = tf.Variable(tf.constant(0.1,shape=[384],dtype=tf.float32))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bais3)\n",
    "    #if train:\n",
    "        # 如果是训练阶段，在这层使用0.5的dropout\n",
    "        #local3 = tf.nn.dropout(local3, 0.2)\n",
    "    #第二层全连接层\n",
    "    weight4 = variable_with_weight_loss([384,192],std=0.04,w1=0.004)\n",
    "    bais4 = tf.Variable(tf.constant(0.1,shape=[192],dtype=tf.float32))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3,weight4)+bais4)\n",
    "\n",
    "    #最后一层\n",
    "    weight5 = variable_with_weight_loss([192,10],std=1/192.0,w1=0)\n",
    "    bais5 = tf.Variable(tf.constant(0.0,shape=[10],dtype=tf.float32))\n",
    "    \n",
    "    logits = tf.add(tf.matmul(local4,weight5),bais5)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 定义TFRecord解析函数\n",
    "def parser(record):\n",
    "    print(record)\n",
    "    features = tf.parse_single_example(record,\n",
    "                                       features={\n",
    "                                           'image' : tf.FixedLenFeature([], tf.string),\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                       })\n",
    "    # 获取图片数据\n",
    "    images = tf.decode_raw(features['image'], tf.uint8)\n",
    "    \n",
    "    # tf.train.shuffle_batch必须确定shape\n",
    "    images = tf.reshape(images, [24*24*3])\n",
    "    \n",
    "    # 图片预处理\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    images = tf.subtract(images, 0.5)\n",
    "    images = tf.multiply(images, 2.0)\n",
    "    # 获取label\n",
    "    labels = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "Tensor(\"arg0:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #设置最大迭代次数\n",
    "    max_steps = 60001\n",
    "    #设置每次训练的数据大小\n",
    "    batch_size = 100\n",
    "    num_classes = 10\n",
    "    epochs = 16\n",
    "    buffer_size = 1000\n",
    "    WIDTH = 24 \n",
    "    HEIGHT = 24\n",
    "    CHANNELS = 3\n",
    "    BATCH_SIZE = 100\n",
    "    LEARNING_RATE_DECAY = 0.96\n",
    "    LEARNING_RATE_BASE = 0.001\n",
    "    # tfrecord文件存放路径\n",
    "    TFRECORD_FILE_TRAIN = ['captcha/train.tfrecords']\n",
    "    TFRECORD_FILE_TEST = ['captcha/test.tfrecords']\n",
    "    test_dir = 'D:/Develop/project/python/tensorflow/ten/param/captcha/images/test'\n",
    "    train_dir = 'D:/Develop/project/python/tensorflow/ten/param/captcha/images/train'\n",
    "    #获取数据增强后的训练集数据\n",
    "    #images_train,labels_train = load_files(train_dir,num_classes,24*24*3)\n",
    "    # 全局步数\n",
    "    global_step = tf.Variable(0, trainable=False) \n",
    "    \n",
    "    # 创建训练集和测试集对象 使用 map() 进行解析\n",
    "    dataset_train =  tf.data.TFRecordDataset(TFRECORD_FILE_TRAIN)\n",
    "    dataset_test =  tf.data.TFRecordDataset(TFRECORD_FILE_TEST)\n",
    "    dataset_train = dataset_train.map(parser)\n",
    "    dataset_test = dataset_test.map(parser)\n",
    "    dataset_train = dataset_train.repeat().shuffle(buffer_size).batch(batch_size)\n",
    "    # 创建迭代器\n",
    "    iterator_train = dataset_train.make_one_shot_iterator()\n",
    "    iterator_test = dataset_test.make_one_shot_iterator()\n",
    "    # 创建遍历器\n",
    "    train_next_batch  = iterator_train.get_next()\n",
    "    test_next_batch = iterator_test.get_next()\n",
    "        \n",
    "    \n",
    "    #定义模型的输入和输出数据\n",
    "    image_holder = tf.placeholder(dtype=tf.float32,shape=[batch_size,24*24*3])\n",
    "    label_holder = tf.placeholder(dtype=tf.int32,shape=[batch_size])\n",
    "    image_holder_reshape = tf.reshape(image_holder, [-1, WIDTH, HEIGHT, CHANNELS])\n",
    "    logits = forward(image_holder_reshape,True)\n",
    "    # 学习率\n",
    "    lr = tf.Variable(1e-3, dtype=tf.float32)\n",
    "\n",
    "    # 设置指数下降学习率\n",
    "    lr_decay = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True) \n",
    "    #获取损失函数\n",
    "    loss = loss_func(logits,label_holder)\n",
    "\n",
    "    #设置优化算法使得成本最小\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
    "    #获取最高类的分类准确率，取top1作为衡量标准\n",
    "    #top_k_op = tf.nn.in_top_k(logits,label_holder,1)\n",
    "    # 计算准确率\n",
    "    # 把标签转成one_hot的形式\n",
    "    one_hot_labels = tf.one_hot(indices=tf.cast(label_holder, tf.int32), depth=num_classes)\n",
    "    correct_prediction = tf.equal(tf.argmax(one_hot_labels,1),tf.argmax(logits,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "#     #计算批次                           \n",
    "#     batches_count_train = int(math.ceil(images_train.shape[0] / batch_size))\n",
    "#     remainder_train = images_train.shape[0] % batch_size \n",
    "#     batches_count_test = int(math.ceil(images_test.shape[0] / batch_size))\n",
    "#     remainder_test = images_test.shape[0] % batch_size\n",
    "    \n",
    "\n",
    "    #创建会话\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    # 用于保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    #开始训练\n",
    "    # 图像增强队列\n",
    "    #tf.train.start_queue_runners()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "        images_batch,b_label = sess.run(train_next_batch)\n",
    "        print(\"images_batch shape\",b_image.shape)\n",
    "        #获取一个批次的数据和标签\n",
    "        #images_batch,labels_batch = sess.run([images_train,labels_train])\n",
    "        b_image, b_label = sess.run([images_train, labels_train])\n",
    "        _,loss_value,step_g = sess.run([train_step,loss,global_step],feed_dict={image_holder:b_image,\n",
    "                                                                 label_holder:b_label})\n",
    "        \n",
    "        #获取计算时间\n",
    "        duration = time.time() - start_time\n",
    "        # 计算测试集准确率\n",
    "        images_b,labels_b = sess.run([images_batch_test,labels_batch_test])\n",
    "        #print(\"1\",images_batch_test.shape)\n",
    "        acc,loss_value_test = sess.run([accuracy,loss],feed_dict={image_holder:images_b,\n",
    "                                                                 label_holder:labels_b}) \n",
    "        #计算每秒处理多少张图片\n",
    "        per_images_second = batch_size / duration\n",
    "        #获取时间\n",
    "        sec_per_batch = float(duration)\n",
    "        # 降低学习率\n",
    "        lr_decay_val = sess.run(lr_decay) \n",
    "        sess.run(tf.assign(lr, lr_decay_val))     \n",
    "        learning_rate = sess.run(lr)    \n",
    "        sys.stdout.write('\\r>> Step:%d,  duration:%.3f,  per_images_second:%.2f,  loss:%.3f,  accuracy:%.3f,  test loss:%.3f,  learning_rate:%.5f' % \n",
    "                         (step_g,duration,per_images_second,loss_value,acc,loss_value_test,learning_rate))\n",
    "        sys.stdout.flush() \n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            # 训练完成后保存训练模型\n",
    "            saver.save(sess, \"./captcha/models/crack_captcha.model\", global_step=global_step, meta_graph_suffix='meta', write_meta_graph=True) \n",
    "            tf.train.write_graph(sess.graph_def, \"./captcha/models\", \"crack_captcha.pb\", False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
