{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameter summary 统计函数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) \n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "初始化权重函数\n",
    "'''\n",
    "def variable_with_weight_loss(shape, std, w1):\n",
    "    var = tf.Variable(tf.truncated_normal(shape,stddev=std),dtype=tf.float32)\n",
    "    \n",
    "    if w1 is not None :\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name=\"weight_loss\")\n",
    "        tf.add_to_collection(\"losses\",weight_loss)\n",
    "    return var\n",
    "\n",
    "'''\n",
    "损失函数\n",
    "'''\n",
    "def loss_func(logits,labels):\n",
    "    labels = tf.cast(labels,tf.int32)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                           labels=labels,name=\"cross_entropy_per_example\")\n",
    "    cross_entropy_mean = tf.reduce_mean(tf.reduce_sum(cross_entropy))\n",
    "    tf.add_to_collection(\"losses\",cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection(\"losses\"),name=\"total_loss\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def forward(image_holder,train):\n",
    "    \n",
    "    #设计第一层卷积\n",
    "    # 第一个卷积层\n",
    "    with tf.name_scope('conv_layer1'):\n",
    "        with tf.name_scope('W_conv1'):\n",
    "            weight1 = variable_with_weight_loss(shape=[5,5,3,64],std=5e-2,w1=0)\n",
    "            variable_summaries(weight1)\n",
    "            kernel1 = tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding=\"SAME\")\n",
    "        with tf.name_scope('b_conv1'):\n",
    "            bais1 = tf.Variable(tf.constant(0.0,dtype=tf.float32,shape=[64]))\n",
    "            variable_summaries(bais1)\n",
    "            conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bais1))\n",
    "        with tf.name_scope('L1_pool'):\n",
    "            pool1 = tf.nn.max_pool(conv1,[1,2,2,1],[1,2,2,1],padding=\"SAME\")\n",
    "        with tf.name_scope('L1_norml'):\n",
    "            norm1 = tf.nn.lrn(pool1,4,bias=1.0,alpha=0.001 / 9,beta=0.75)\n",
    "\n",
    "    #设计第二层卷积\n",
    "    with tf.name_scope('conv_layer2'):\n",
    "        with tf.name_scope('W_conv2'):\n",
    "            weight2 = variable_with_weight_loss(shape=[5,5,64,64],std=5e-2,w1=0)\n",
    "            variable_summaries(weight2)\n",
    "            kernel2 = tf.nn.conv2d(norm1,weight2,[1,1,1,1],padding=\"SAME\")\n",
    "        with tf.name_scope('b_conv2'):\n",
    "            bais2 = tf.Variable(tf.constant(0.1,dtype=tf.float32,shape=[64]))\n",
    "            variable_summaries(bais2)\n",
    "            conv2 = tf.nn.relu(tf.nn.bias_add(kernel2,bais2))\n",
    "        with tf.name_scope('L2_pool'):\n",
    "            norm2 = tf.nn.lrn(conv2,4,bias=1.0,alpha=0.01 / 9,beta=0.75)\n",
    "        with tf.name_scope('L2_norml'):\n",
    "            pool2 = tf.nn.max_pool(norm2,[1,3,3,1],[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "    #第一层全连接层\n",
    "    # 把pool2从三维张量变为二维张量\n",
    "    with tf.name_scope('fc_layer3'):\n",
    "        with tf.name_scope('W_fc3'):\n",
    "            pool_shape = pool2.get_shape().as_list()         # 得到pool2 输出矩阵的维度存入list中\n",
    "            nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]      # 提取特征的长度宽度深度 相乘得所有特征点的个数\n",
    "            reshape = tf.reshape(pool2, [pool_shape[0], nodes])       # pool_shape[0]是一个batch的值\n",
    "            #reshape = tf.reshape(pool2,[batch_size,-1])\n",
    "            #dim = reshape.get_shape()[1].value\n",
    "            weight3 = variable_with_weight_loss([nodes,384],std=0.04,w1=0.004)\n",
    "            variable_summaries(weight3)\n",
    "        with tf.name_scope('b_fc3'):\n",
    "            bais3 = tf.Variable(tf.constant(0.1,shape=[384],dtype=tf.float32))\n",
    "            variable_summaries(bais3)\n",
    "            local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bais3)\n",
    "            #if train:\n",
    "        # 如果是训练阶段，在这层使用0.5的dropout\n",
    "        #local3 = tf.nn.dropout(local3, 0.2)\n",
    "    #第二层全连接层\n",
    "    with tf.name_scope('fc_layer4'):\n",
    "        with tf.name_scope('W_fc4'):\n",
    "            weight4 = variable_with_weight_loss([384,192],std=0.04,w1=0.004)\n",
    "            variable_summaries(weight4)\n",
    "        with tf.name_scope('b_fc4'):\n",
    "            bais4 = tf.Variable(tf.constant(0.1,shape=[192],dtype=tf.float32))\n",
    "            variable_summaries(bais4)\n",
    "            local4 = tf.nn.relu(tf.matmul(local3,weight4)+bais4)\n",
    "\n",
    "    #最后一层\n",
    "    with tf.name_scope('fc_layer5'):\n",
    "        with tf.name_scope('W_fc5'):\n",
    "            weight5 = variable_with_weight_loss([192,10],std=1/192.0,w1=0)\n",
    "            variable_summaries(weight5)\n",
    "        with tf.name_scope('b_fc5'):\n",
    "            bais5 = tf.Variable(tf.constant(0.0,shape=[10],dtype=tf.float32))\n",
    "            variable_summaries(bais4)\n",
    "            logits = tf.add(tf.matmul(local4,weight5),bais5)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 从tfrecord读出数据\n",
    "def read_and_decode(filename):\n",
    "    # 根据文件名生成一个队列\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    reader = tf.TFRecordReader()\n",
    "    # 返回文件名和文件\n",
    "    _, serialized_example = reader.read(filename_queue)   \n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'image' : tf.FixedLenFeature([], tf.string),\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                       })\n",
    "    # 获取图片数据\n",
    "    images = tf.decode_raw(features['image'], tf.uint8)\n",
    "    \n",
    "    # tf.train.shuffle_batch必须确定shape\n",
    "    images = tf.reshape(images, [24*24*3])\n",
    "    \n",
    "    # 图片预处理\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    images = tf.subtract(images, 0.5)\n",
    "    images = tf.multiply(images, 2.0)\n",
    "    # 获取label\n",
    "    labels = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-74b350acff5e>:6: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-74b350acff5e>:7: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-5-d75d1b9e8f04>:30: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From <ipython-input-5-d75d1b9e8f04>:94: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from train_saver/digits\\crack_captcha.model-5000\n",
      ">> Step:7453,  duration:0.823,  per_images_second:121.58,  loss:0.458,  accuracy:0.790,  test loss:273.527,  learning_rate:0.00005"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d75d1b9e8f04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# 降低学习率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mlr_decay_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_decay_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         sys.stdout.write('\\r>> Step:%d,  duration:%.3f,  per_images_second:%.2f,  loss:%.3f,  accuracy:%.3f,  test loss:%.3f,  learning_rate:%.5f' % \n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #设置最大迭代次数\n",
    "    max_steps = 20001\n",
    "    #设置每次训练的数据大小\n",
    "    batch_size = 100\n",
    "    num_classes = 10\n",
    "    epochs = 16\n",
    "    buffer_size = 1000\n",
    "    WIDTH = 24 \n",
    "    HEIGHT = 24\n",
    "    CHANNELS = 3\n",
    "    BATCH_SIZE = 100\n",
    "    LEARNING_RATE_DECAY = 0.96\n",
    "    LEARNING_RATE_BASE = 0.001\n",
    "    # tfrecord文件存放路径\n",
    "    TFRECORD_FILE_TRAIN = 'train_images/train.tfrecords'\n",
    "    TFRECORD_FILE_TEST = 'train_images/test.tfrecords'\n",
    "    MODEL_SAVE_PATH = 'train_saver/digits'\n",
    "    #获取数据增强后的训练集数据\n",
    "    #images_train,labels_train = load_files(train_dir,num_classes,24*24*3)\n",
    "    # 全局步数\n",
    "    global_step = tf.Variable(0, trainable=False) \n",
    "    \n",
    " #获取数据增强后的训练集数据\n",
    "    #images_train,labels_train = load_files(train_dir,num_classes,24*24*3)\n",
    "    images_train,labels_train = read_and_decode(TFRECORD_FILE_TRAIN)\n",
    "    image_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [images_train, labels_train], batch_size = BATCH_SIZE,\n",
    "        capacity = 50000, min_after_dequeue=10000)\n",
    "   \n",
    "    #获取裁剪后的测试数据\n",
    "    #images_test,labels_test = load_files(test_dir,num_classes,24*24*3)\n",
    "    images_test,labels_test = read_and_decode(TFRECORD_FILE_TEST)\n",
    "    images_batch_test, labels_batch_test = tf.train.shuffle_batch(\n",
    "        [images_test, labels_test], batch_size = BATCH_SIZE,\n",
    "        capacity = 50000, min_after_dequeue=10000)\n",
    "        \n",
    "    \n",
    "    #定义模型的输入和输出数据\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        with tf.name_scope(\"image_hoder\"):\n",
    "            image_holder = tf.placeholder(dtype=tf.float32,shape=[batch_size,24*24*3])\n",
    "            label_holder = tf.placeholder(dtype=tf.int32,shape=[batch_size])\n",
    "            image_holder_reshape = tf.reshape(image_holder, [-1, WIDTH, HEIGHT, CHANNELS])\n",
    "    with tf.name_scope(\"learning_rate\"):\n",
    "        # 学习率\n",
    "        lr = tf.Variable(1e-3, dtype=tf.float32)\n",
    "        tf.summary.scalar('learning_rate',lr)\n",
    "        # 设置指数下降学习率\n",
    "        lr_decay = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            BATCH_SIZE,\n",
    "            LEARNING_RATE_DECAY,\n",
    "            staircase=True) \n",
    "    \n",
    "    \n",
    "\n",
    "    logits = forward(image_holder_reshape,True)\n",
    "\n",
    "    #设置优化算法使得成本最小\n",
    "        # 定义优化器和训练op\n",
    "    with tf.name_scope('optimizer'):\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = loss_func(logits,label_holder)\n",
    "            tf.summary.scalar('loss',loss)\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(loss,global_step=global_step)\n",
    "    #获取最高类的分类准确率，取top1作为衡量标准\n",
    "    #top_k_op = tf.nn.in_top_k(logits,label_holder,1)\n",
    "    # 计算准确率\n",
    "    # 把标签转成one_hot的形式\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            one_hot_labels = tf.one_hot(indices=tf.cast(label_holder, tf.int32), depth=num_classes)\n",
    "            correct_prediction = tf.equal(tf.argmax(one_hot_labels,1),tf.argmax(logits,1))\n",
    "        with tf.name_scope('accuracy'):    \n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "            tf.summary.scalar('accuracy',accuracy)\n",
    "#     #计算批次                           \n",
    "#     batches_count_train = int(math.ceil(images_train.shape[0] / batch_size))\n",
    "#     remainder_train = images_train.shape[0] % batch_size \n",
    "#     batches_count_test = int(math.ceil(images_test.shape[0] / batch_size))\n",
    "#     remainder_test = images_test.shape[0] % batch_size\n",
    "    \n",
    "\n",
    "    #创建会话\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    # 用于保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    #开始训练\n",
    "    # 图像增强队列\n",
    "    tf.train.start_queue_runners()\n",
    "\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    \n",
    "    #合并所有的summary\n",
    "    merged = tf.summary.merge_all()\n",
    "    ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    for step in range(max_steps):\n",
    "        start_time = time.time()\n",
    "        #获取一个批次的数据和标签\n",
    "        #images_batch,labels_batch = sess.run([images_train,labels_train])\n",
    "        b_image, b_label = sess.run([image_batch, label_batch])\n",
    "        _,loss_value,summary,step_g = sess.run([train_step,loss,merged,global_step],feed_dict={image_holder:b_image,\n",
    "                                                                 label_holder:b_label})\n",
    "        \n",
    "        #获取计算时间\n",
    "        duration = time.time() - start_time\n",
    "        # 计算测试集准确率\n",
    "        images_b,labels_b = sess.run([images_batch_test,labels_batch_test])\n",
    "        #print(\"1\",images_batch_test.shape)\n",
    "        acc,loss_value_test = sess.run([accuracy,loss],feed_dict={image_holder:images_b,\n",
    "                                                                 label_holder:labels_b}) \n",
    "        tf.summary.scalar('accuracy_test',acc)\n",
    "        #计算每秒处理多少张图片\n",
    "        per_images_second = batch_size / duration\n",
    "        #获取时间\n",
    "        sec_per_batch = float(duration)\n",
    "        # 降低学习率\n",
    "        lr_decay_val = sess.run(lr_decay) \n",
    "        sess.run(tf.assign(lr, lr_decay_val))     \n",
    "        learning_rate = sess.run(lr)    \n",
    "        sys.stdout.write('\\r>> Step:%d,  duration:%.3f,  per_images_second:%.2f,  loss:%.3f,  accuracy:%.3f,  test loss:%.3f,  learning_rate:%.5f' % \n",
    "                         (step_g,duration,per_images_second,loss_value,acc,loss_value_test,learning_rate))\n",
    "        sys.stdout.flush() \n",
    "        writer.add_summary(summary,step)\n",
    "        if step_g % 500 == 0:\n",
    "            # 训练完成后保存训练模型\n",
    "            saver.save(sess, \"./train_saver/digits/crack_captcha.model\", global_step=global_step, meta_graph_suffix='meta', write_meta_graph=True) \n",
    "            tf.train.write_graph(sess.graph_def, \"./train_saver/digits\", \"crack_captcha.pb\", False)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
