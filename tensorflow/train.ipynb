{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage.data\n",
    "\n",
    "SIZE = 576\n",
    "#SIZE = 1728\n",
    "WIDTH = 24\n",
    "HEIGHT = 24\n",
    "CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "CONV1_SIZE = 8\n",
    "CONV2_KERNEL_NUM = 32\n",
    "CONV2_SIZE = 5\n",
    "CONV2_KERNEL_NUM = 64\n",
    "CONV3_SIZE = 5\n",
    "CONV3_KERNEL_NUM = 64\n",
    "iterations = 101\n",
    "\n",
    "\n",
    "SAVER_DIR = \"train_saver/digits/\"\n",
    "TRAIN_DIR = \"\"\n",
    "LETTERS_DIGITS = (\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\")\n",
    "\n",
    "TRAIN_PATH = \"train_images/train\"\n",
    "TEST_PATH = \"train_images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# path 输入需要读取的文件夹路径 文件夹类的标签由文件顺序从0-num_class\n",
    "# num_class 输入文件需要的分类数量\n",
    "# size 图片的大小 size = width*height*chanenls\n",
    "# 输出 input_images 图片数据二维数组 大小为 [file_count,size],input_label 标签二维数组 大小为 [file_count,num_class]\n",
    "def load_files(path,num_class,size):\n",
    "    time_begin = time.time()\n",
    "\n",
    "    input_count = 0\n",
    "    for i in range(0,num_class):\n",
    "        dir = '%s/%s/' % (path,i)           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "        for rt, dirs, files in os.walk(dir):\n",
    "            for filename in files:\n",
    "                input_count += 1\n",
    "\n",
    "    # 定义对应维数和各维长度的数组\n",
    "    input_images = np.array([[0]*size for i in range(input_count)])\n",
    "    input_labels = np.array([[0]*num_class for i in range(input_count)])\n",
    "\n",
    "    index = 0\n",
    "    for i in range(0,num_class):\n",
    "        dir = '%s/%s/' % (path,i)           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "        for rt, dirs, files in os.walk(dir):\n",
    "            for filename in files:\n",
    "                if(filename.endswith('.bmp')):\n",
    "                    filename = dir + filename\n",
    "                    img = cv2.imread(filename)\n",
    "                    imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "                    imgResize = cv2.resize(imgGray,(24,24))\n",
    "                    # canny = cv2.Canny(imgResize, 50, 150)\n",
    "                    # binary = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 25, 10)\n",
    "                    # 转维度由 一维数据\n",
    "                    # img = skimage.data.imread(filename)\n",
    "                    # imgResize = skimage.transform.resize(img, (24, 24), mode='constant')\n",
    "                    imgReshape = imgResize.reshape(1,SIZE)\n",
    "                    input_images[index] = imgReshape\n",
    "                    input_labels[index][i] = 1\n",
    "                    index += 1\n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print(input_images[0][:])\n",
    "    print(\"读取的文件数为\",input_count)\n",
    "    print(\"读取图片文件耗费时间：%d秒\" % time_elapsed)\n",
    "    return input_images,input_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 64 and 32 for 'conv_layer3/L3_pool/Conv2D' (op: 'Conv2D') with input shapes: [?,6,6,64], [5,5,32,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 64 and 32 for 'conv_layer3/L3_pool/Conv2D' (op: 'Conv2D') with input shapes: [?,6,6,64], [5,5,32,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bb05f9cc7909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m            \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m            \u001b[0mpool_strides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m            \u001b[0mL2_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL2_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_conv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_conv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m    \u001b[1;31m# 全连接层 int((WIDTH/2)*(HEIGHT/2)*CHANNELS*64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-bb05f9cc7909>\u001b[0m in \u001b[0;36mconv_layer\u001b[1;34m(inputs, W, b, conv_strides, kernel_size, pool_strides, padding)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 定义卷积函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m    \u001b[0mL1_conv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m    \u001b[0mL1_relu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL1_conv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL1_relu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m   1026\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\python\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 64 and 32 for 'conv_layer3/L3_pool/Conv2D' (op: 'Conv2D') with input shapes: [?,6,6,64], [5,5,32,64]."
     ]
    }
   ],
   "source": [
    " # 定义卷积函数\n",
    "def conv_layer(inputs, W, b, conv_strides, kernel_size, pool_strides, padding):\n",
    "    L1_conv = tf.nn.conv2d(inputs, W, strides=conv_strides, padding=padding)\n",
    "    L1_relu = tf.nn.relu(L1_conv + b)\n",
    "    return tf.nn.max_pool(L1_relu, ksize=kernel_size, strides=pool_strides, padding='SAME')\n",
    " \n",
    "# 定义全连接层函数\n",
    "def full_connect(inputs, W, b):\n",
    "    return tf.nn.relu(tf.matmul(inputs, W) + b)\n",
    "\n",
    "#parameter summary\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) \n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram',var)\n",
    "# 定义输入节点，对应于图片像素值矩阵集合和图片标签(即所代表的数字)\n",
    "with tf.name_scope('input'):\n",
    "    with tf.name_scope('input_x'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, SIZE])\n",
    "    with tf.name_scope('input_y'):\n",
    "        y = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])\n",
    "    with tf.name_scope('input_leaning'):\n",
    "        lr = tf.Variable(0.0001,dtype=tf.float32)\n",
    "x_image = tf.reshape(x, [-1, WIDTH, HEIGHT, CHANNELS])\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # 第一个卷积层\n",
    "    with tf.name_scope('conv_layer1'):\n",
    "        with tf.name_scope('W_conv1'):\n",
    "            W_conv1 = tf.Variable(tf.truncated_normal([CONV1_SIZE, CONV1_SIZE, CHANNELS, 32], stddev=0.1), name=\"W_conv1\")\n",
    "            variable_summaries(W_conv1)\n",
    "        with tf.name_scope('b_conv1'):\n",
    "            b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]), name=\"b_conv1\")\n",
    "            variable_summaries(b_conv1)\n",
    "        with tf.name_scope('L1_pool'):\n",
    "            conv_strides = [1, 1, 1, 1]\n",
    "            kernel_size = [1, 2, 2, 1]\n",
    "            pool_strides = [1, 2, 2, 1]\n",
    "            L1_pool = conv_layer(x_image, W_conv1, b_conv1, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "    # 第二个卷积层\n",
    "    with tf.name_scope('conv_layer2'):\n",
    "        with tf.name_scope('W_conv2'):\n",
    "            W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1), name=\"W_conv2\")\n",
    "            variable_summaries(W_conv2)\n",
    "        with tf.name_scope('b_conv2'):\n",
    "            b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b_conv2\")\n",
    "            variable_summaries(b_conv2)\n",
    "        with tf.name_scope('L2_pool'):\n",
    "            conv_strides = [1, 1, 1, 1]\n",
    "            kernel_size = [1, 2, 2, 1]\n",
    "            pool_strides = [1, 2, 2, 1]\n",
    "            L2_pool = conv_layer(L1_pool, W_conv2, b_conv2, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "    # 第三个卷积层\n",
    "    with tf.name_scope('conv_layer3'):\n",
    "        with tf.name_scope('W_conv3'):\n",
    "            W_conv3 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1), name=\"W_conv3\")\n",
    "            variable_summaries(W_conv2)\n",
    "        with tf.name_scope('b_conv3'):\n",
    "            b_conv3 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b_conv3\")\n",
    "            variable_summaries(b_conv3)\n",
    "        with tf.name_scope('L3_pool'):\n",
    "            conv_strides = [1, 1, 1, 1]\n",
    "            kernel_size = [1, 1, 1, 1]\n",
    "            pool_strides = [1, 1, 1, 1]\n",
    "            L2_pool = conv_layer(L2_pool, W_conv3, b_conv3, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "            \n",
    "    # 全连接层 int((WIDTH/2)*(HEIGHT/2)*CHANNELS*64)\n",
    "    with tf.name_scope('fc_layer1'):\n",
    "        with tf.name_scope('W_fc1'):\n",
    "            W_fc1 = tf.Variable(tf.truncated_normal([12*12*64, 1024], stddev=0.1), name=\"W_fc1\")\n",
    "            variable_summaries(W_fc1)\n",
    "        with tf.name_scope('b_fc1'):\n",
    "            b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b_fc1\")\n",
    "            variable_summaries(b_fc1)\n",
    "        with tf.name_scope('h_fc1'):\n",
    "            h_pool2_flat = tf.reshape(L2_pool, [-1, 12*12*64])\n",
    "            h_fc1 = full_connect(h_pool2_flat, W_fc1, b_fc1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "    # readout层\n",
    "    with tf.name_scope('fc_layer2'):\n",
    "        with tf.name_scope('W_fc2'):\n",
    "            W_fc2 = tf.Variable(tf.truncated_normal([1024, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "            variable_summaries(W_fc2)\n",
    "        with tf.name_scope('b_fc2'):\n",
    "            b_fc2 = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "            variable_summaries(b_fc2)\n",
    "\n",
    "\n",
    "    # 定义优化器和训练op\n",
    "    with tf.name_scope('optimizer'):\n",
    "        with tf.name_scope('y_conv'):\n",
    "            y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "            tf.summary.scalar('cross_entropy',cross_entropy)\n",
    "        with tf.name_scope('train_step'):\n",
    "            train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "    #求准确率\n",
    "    with tf.name_scope('correct'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('accuracy',accuracy)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    time_begin = time.time()\n",
    "\n",
    "    print (\"一共读取了 %s 个训练图像， %s 个标签\" % (input_count, input_count))\n",
    "\n",
    "    # 设置每次训练op的输入个数和迭代次数，这里为了支持任意图片总数，定义了一个余数remainder，譬如，如果每次训练op的输入个数为60，图片总数为150张，则前面两次各输入60张，最后一次输入30张（余数30）\n",
    "    batch_size = 60\n",
    "    iterations = iterations\n",
    "    batches_count = int(input_count / batch_size)\n",
    "    remainder = input_count % batch_size\n",
    "    print (\"训练数据集分成 %s 批, 前面每批 %s 个数据，最后一批 %s 个数据\" % (batches_count+1, batch_size, remainder))\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    \n",
    "    #合并所有的summary\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 初始化saver\n",
    "    saver = tf.train.Saver()   \n",
    "    # 执行训练迭代\n",
    "    for it in range(iterations):\n",
    "        # 降低学习率\n",
    "        sess.run(tf.assign(lr,0.00015 * (0.95 ** it)))\n",
    "        # 这里的关键是要把输入数组转为np.array\n",
    "        \n",
    "        for n in range(batches_count):\n",
    "            summary,_ = sess.run([merged,train_step],feed_dict={x: input_images[n*batch_size:(n+1)*batch_size], y: input_labels[n*batch_size:(n+1)*batch_size], keep_prob: 0.8})\n",
    "#             train_step.run(feed_dict={x: input_images[n*batch_size:(n+1)*batch_size], y: input_labels[n*batch_size:(n+1)*batch_size], keep_prob: 0.5})\n",
    "        if remainder > 0:\n",
    "            start_index = batches_count * batch_size;\n",
    "            summary,_ = sess.run([merged,train_step],feed_dict={x: input_images[start_index:input_count], y: input_labels[start_index:input_count], keep_prob: 0.8})\n",
    "#             train_step.run(feed_dict={x: input_images[start_index:input_count-1], y: input_labels[start_index:input_count-1], keep_prob: 0.5})\n",
    "        learning_rate = sess.run(lr)\n",
    "        # 每完成五次迭代，判断准确度是否已达到100%，达到则退出迭代循环\n",
    "        iterate_accuracy = 0\n",
    "        if it%5 == 0:\n",
    "            iterate_accuracy = accuracy.eval(feed_dict={x: input_images, y: input_labels, keep_prob: 1.0})\n",
    "            print ('第 %d 次训练迭代: 准确率 %0.5f%% 学习率 %s' % (it, iterate_accuracy*100,str(learning_rate)))\n",
    "            if iterate_accuracy >= 0.9999 and it >= iterations:\n",
    "                break;\n",
    "        writer.add_summary(summary,it)\n",
    "    print ('完成训练!')\n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print (\"训练耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "    # 保存训练结果\n",
    "    if not os.path.exists(SAVER_DIR):\n",
    "        print ('不存在训练数据保存目录，现在创建保存目录')\n",
    "        os.makedirs(SAVER_DIR)\n",
    "             \n",
    "    saver_path = saver.save(sess, \"%smodel.ckpt\"%(SAVER_DIR),global_step=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
